{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This code is for determining my initial list of stocks. I have a twitter account that \n",
    "##follows 150 penny stock focused day/swing traders each with at least 1000 followers. \n",
    "\n",
    "#I have generally found that on Sundays everyone is talking about what stocks they like for\n",
    "#the week. This code collects all of the tweets from that timeline on Sunday and exports \n",
    "##them to a .json file that you need to name\n",
    "#(fine tune how far back in time it goes by setting the number of pages, 3 is generally\n",
    "#around the right amount when i run this at 11 pm EST). \n",
    "#Then it reads the .json file (need to set file name for the read) and tallies up how \n",
    "#many times each stock got mentioned on the timeline and exports a list of \n",
    "#stock name and mentions. A lot of this initial chunk i stitched together from various places\n",
    "#on the internet and did not write\n",
    "##In the next step of this code, it takes the list of stocks and using the WeBull API checks\n",
    "# to see if they are available for trading, and also filters them to make sure they aren't something\n",
    "##super legit and non-volatile.\n",
    "##it then automatically calculates several machine learning model features and exports to\n",
    "# a spreadsheet \n",
    "##I am pretty sure you need to be logged into webull in order to access the webull API data. \n",
    "from tweepy import OAuthHandler\n",
    "import operator \n",
    "import json\n",
    "import nltk\n",
    "import tweepy\n",
    "from tweepy import Cursor\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import re\n",
    "##these are for accessing the twitter account, there is rate limiting on this i forget \n",
    "##what the rules are exactly but look them up if you are gonna try and get more than ~1,000\n",
    "#tweets a couple times in a row I'd say (will get kicked if you break there rules too much)\n",
    "consumer_key = \"XXXXXXXXXXXXXXXXXX\"\n",
    "consumer_secret = \"XXXXXXXXXXXXXXXX\"\n",
    "access_token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "access_secret = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "#dont want to put the keys on the internet\n",
    "# Creating the authentication object\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "from webull import webull\n",
    "import xlsxwriter\n",
    "api = tweepy.API(auth)\n",
    "wb = webull()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ####################################set export file name for json here\n",
    "    with open ('home_timelineTESTTEST.json1', 'w') as f:\n",
    "        ##################################################################fine tune how many \n",
    "        ###tweets you are reading here \n",
    "        for page in tweepy.Cursor(api.home_timeline, count = 200).pages(3):\n",
    "        ##################################################################\n",
    "            for status in page:\n",
    "                f.write(json.dumps(status._json)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('$USWS', 21), ('$NAKD', 21), ('$CCIV', 20), ('$TTOO', 13), ('$TMBR', 10), ('$SNDL', 9), ('$ZOM', 9), ('$CLOV', 8), ('$JUPW', 8), ('$LMFA', 7), ('$ACST', 7), ('$ISR', 6), ('$BNGO', 6), ('$NIO', 6), ('$AVGR', 6), ('$XSPA', 6), ('$TSLA', 4), ('$GOVX', 4), ('$THMO', 4), ('$XPEL', 4), ('$ZM', 3), ('$SIFY', 3), ('$TRXC', 3), ('$MESO', 3), ('$GEVO', 3), ('$NVDA', 3), ('$CTRM', 3), ('$LEAP', 3), ('$AIKI', 2), ('$DFFN', 2), ('$NSPR', 2), ('$RIOT', 2), ('$CBMJ', 2), ('$KMPH', 2), ('$NBY', 2), ('$LLY', 2), ('$BTWN', 2), ('$CRBP', 2), ('$EXPC', 2), ('$SRPT', 2), ('$TLRY', 2), ('$SIGL', 2), ('$CIDM', 2), ('$RIDE', 2), ('$SLGG', 2), ('$NTEC', 2), ('$ACB', 2), ('$IDEX', 2), ('$OCGN', 2), ('$FB', 2), ('$TAK', 2), ('$GHSI', 2), ('$BRQS', 2), ('$CHNR', 2), ('$ELTK', 2), ('$TGC', 2), ('$BW', 1), ('$AEZS', 1), ('$NPA', 1), ('$GRNQ', 1), ('$UVXY', 1), ('$TSNP', 1), ('$PYPL', 1), ('$SQ', 1), ('$MGNI', 1), ('$MARA', 1), ('$NMRD', 1), ('$PENN', 1), ('$STTH', 1), ('$UPIN', 1), ('$PGAS', 1), ('$INUV', 1), ('$MSOS', 1), ('$KBWB', 1), ('$EXAS', 1), ('$IPOC', 1), ('$GMEV', 1), ('$GSL', 1), ('$NOVN', 1), ('$ATHE', 1), ('$BORR', 1), ('$UPWK', 1), ('$GME', 1), ('$FCEL', 1), ('$NGA', 1), ('$OZSC', 1), ('$GRWG', 1), ('$QS', 1), ('$SYN', 1), ('$SOS', 1), ('$ADXS', 1), ('$TYME', 1), ('$LPCN', 1), ('$PLUG', 1), ('$BIDU', 1), ('$PHIO', 1), ('$KERN', 1), ('$NBRV', 1), ('$PEN', 1), ('$BA', 1), ('$1B', 1), ('$DMTK', 1), ('$PLRTF', 1), ('$NIHK', 1), ('$WTER', 1), ('$SEII', 1), ('$TPTW', 1), ('$CANF', 1), ('$SIML', 1), ('$BFT', 1), ('$SALM', 1), ('$GIK', 1), ('$AWX', 1), ('$SPY', 1), ('$FUV', 1), ('$DDD', 1), ('$XONE', 1), ('$SUNW', 1), ('$PECK', 1), ('$CREG', 1), ('$FTFT', 1), ('$FSR', 1), ('$CRON', 1), ('$APHA', 1), ('$EQOS', 1), ('$AMD', 1), ('$INTC', 1), ('$SNAP', 1), ('$WMT', 1), ('$TWTR', 1), ('$ACAM', 1), ('$QLGN', 1), ('$CEMI', 1), ('$HUGE', 1), ('$PIXY', 1), ('$RAVE', 1), ('$ZYNE', 1), ('$CGC', 1), ('$HEXO', 1), ('$TTNP', 1), ('$PSTH', 1), ('$USWSW', 1), ('$KRUS', 1), ('$STAF', 1), ('$SNMP', 1), ('$ONTX', 1), ('$CVIV', 1), ('$INVE', 1), ('$TLSS', 1), ('$PVDG', 1), ('$REED', 1), ('$PED', 1), ('$BOX', 1), ('$DPW', 1), ('$ALRN', 1), ('$PI', 1), ('$SHIP', 1), ('$EKSO', 1), ('$DTII', 1), ('$IGEX', 1), ('$GRSO', 1), ('$NECA', 1), ('$VNUE', 1), ('$PSAC', 1), ('$VOXX', 1), ('$SNX', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # stonks\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "  \n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str, re.VERBOSE | re.IGNORECASE)\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'I', 'RT', 'DD', 'SPY', 'PR', 'A', 'THIS']\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "######file you want to read \n",
    "fname = 'home_timelineTESTTEST.json1'\n",
    "#######################\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])if len(term)<=6 and term not in stop and term.startswith('$') and term.isupper()]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "        a = count_all.most_common(300)\n",
    "    print(count_all.most_common(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$USWS', 21, '$NAKD', 21, '$CCIV', 20, '$TTOO', 13, '$TMBR', 10, '$SNDL', 9, '$ZOM', 9, '$CLOV', 8, '$JUPW', 8, '$LMFA', 7, '$ACST', 7, '$ISR', 6, '$BNGO', 6, '$NIO', 6, '$AVGR', 6, '$XSPA', 6, '$TSLA', 4, '$GOVX', 4, '$THMO', 4, '$XPEL', 4, '$ZM', 3, '$SIFY', 3, '$TRXC', 3, '$MESO', 3, '$GEVO', 3, '$NVDA', 3, '$CTRM', 3, '$LEAP', 3, '$AIKI', 2, '$DFFN', 2, '$NSPR', 2, '$RIOT', 2, '$CBMJ', 2, '$KMPH', 2, '$NBY', 2, '$LLY', 2, '$BTWN', 2, '$CRBP', 2, '$EXPC', 2, '$SRPT', 2, '$TLRY', 2, '$SIGL', 2, '$CIDM', 2, '$RIDE', 2, '$SLGG', 2, '$NTEC', 2, '$ACB', 2, '$IDEX', 2, '$OCGN', 2, '$FB', 2, '$TAK', 2, '$GHSI', 2, '$BRQS', 2, '$CHNR', 2, '$ELTK', 2, '$TGC', 2, '$BW', 1, '$AEZS', 1, '$NPA', 1, '$GRNQ', 1, '$UVXY', 1, '$TSNP', 1, '$PYPL', 1, '$SQ', 1, '$MGNI', 1, '$MARA', 1, '$NMRD', 1, '$PENN', 1, '$STTH', 1, '$UPIN', 1, '$PGAS', 1, '$INUV', 1, '$MSOS', 1, '$KBWB', 1, '$EXAS', 1, '$IPOC', 1, '$GMEV', 1, '$GSL', 1, '$NOVN', 1, '$ATHE', 1, '$BORR', 1, '$UPWK', 1, '$GME', 1, '$FCEL', 1, '$NGA', 1, '$OZSC', 1, '$GRWG', 1, '$QS', 1, '$SYN', 1, '$SOS', 1, '$ADXS', 1, '$TYME', 1, '$LPCN', 1, '$PLUG', 1, '$BIDU', 1, '$PHIO', 1, '$KERN', 1, '$NBRV', 1, '$PEN', 1, '$BA', 1, '$1B', 1, '$DMTK', 1, '$PLRTF', 1, '$NIHK', 1, '$WTER', 1, '$SEII', 1, '$TPTW', 1, '$CANF', 1, '$SIML', 1, '$BFT', 1, '$SALM', 1, '$GIK', 1, '$AWX', 1, '$SPY', 1, '$FUV', 1, '$DDD', 1, '$XONE', 1, '$SUNW', 1, '$PECK', 1, '$CREG', 1, '$FTFT', 1, '$FSR', 1, '$CRON', 1, '$APHA', 1, '$EQOS', 1, '$AMD', 1, '$INTC', 1, '$SNAP', 1, '$WMT', 1, '$TWTR', 1, '$ACAM', 1, '$QLGN', 1, '$CEMI', 1, '$HUGE', 1, '$PIXY', 1, '$RAVE', 1, '$ZYNE', 1, '$CGC', 1, '$HEXO', 1, '$TTNP', 1, '$PSTH', 1, '$USWSW', 1, '$KRUS', 1, '$STAF', 1, '$SNMP', 1, '$ONTX', 1, '$CVIV', 1, '$INVE', 1, '$TLSS', 1, '$PVDG', 1, '$REED', 1, '$PED', 1, '$BOX', 1, '$DPW', 1, '$ALRN', 1, '$PI', 1, '$SHIP', 1, '$EKSO', 1, '$DTII', 1, '$IGEX', 1, '$GRSO', 1, '$NECA', 1, '$VNUE', 1, '$PSAC', 1, '$VOXX', 1, '$SNX', 1]\n",
      "['$USWS', '$NAKD', '$CCIV', '$TTOO', '$TMBR', '$SNDL', '$ZOM', '$CLOV', '$JUPW', '$LMFA', '$ACST', '$ISR', '$BNGO', '$NIO', '$AVGR', '$XSPA', '$TSLA', '$GOVX', '$THMO', '$XPEL', '$ZM', '$SIFY', '$TRXC', '$MESO', '$GEVO', '$NVDA', '$CTRM', '$LEAP', '$AIKI', '$DFFN', '$NSPR', '$RIOT', '$CBMJ', '$KMPH', '$NBY', '$LLY', '$BTWN', '$CRBP', '$EXPC', '$SRPT', '$TLRY', '$SIGL', '$CIDM', '$RIDE', '$SLGG', '$NTEC', '$ACB', '$IDEX', '$OCGN', '$FB', '$TAK', '$GHSI', '$BRQS', '$CHNR', '$ELTK', '$TGC', '$BW', '$AEZS', '$NPA', '$GRNQ', '$UVXY', '$TSNP', '$PYPL', '$SQ', '$MGNI', '$MARA', '$NMRD', '$PENN', '$STTH', '$UPIN', '$PGAS', '$INUV', '$MSOS', '$KBWB', '$EXAS', '$IPOC', '$GMEV', '$GSL', '$NOVN', '$ATHE', '$BORR', '$UPWK', '$GME', '$FCEL', '$NGA', '$OZSC', '$GRWG', '$QS', '$SYN', '$SOS', '$ADXS', '$TYME', '$LPCN', '$PLUG', '$BIDU', '$PHIO', '$KERN', '$NBRV', '$PEN', '$BA', '$1B', '$DMTK', '$PLRTF', '$NIHK', '$WTER', '$SEII', '$TPTW', '$CANF', '$SIML', '$BFT', '$SALM', '$GIK', '$AWX', '$SPY', '$FUV', '$DDD', '$XONE', '$SUNW', '$PECK', '$CREG', '$FTFT', '$FSR', '$CRON', '$APHA', '$EQOS', '$AMD', '$INTC', '$SNAP', '$WMT', '$TWTR', '$ACAM', '$QLGN', '$CEMI', '$HUGE', '$PIXY', '$RAVE', '$ZYNE', '$CGC', '$HEXO', '$TTNP', '$PSTH', '$USWSW', '$KRUS', '$STAF', '$SNMP', '$ONTX', '$CVIV', '$INVE', '$TLSS', '$PVDG', '$REED', '$PED', '$BOX', '$DPW', '$ALRN', '$PI', '$SHIP', '$EKSO', '$DTII', '$IGEX', '$GRSO', '$NECA', '$VNUE', '$PSAC', '$VOXX', '$SNX']\n",
      "[21, 21, 20, 13, 10, 9, 9, 8, 8, 7, 7, 6, 6, 6, 6, 6, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Just Right ['USWS', 'NAKD', 'CCIV', 'TTOO', 'TMBR', 'SNDL', 'ZOM', 'CLOV', 'JUPW', 'LMFA', 'ACST', 'ISR', 'BNGO', 'AVGR', 'XSPA', 'GOVX', 'THMO', 'SIFY', 'TRXC', 'MESO', 'GEVO', 'CTRM', 'AIKI', 'DFFN', 'NSPR', 'RIOT', 'KMPH', 'NBY', 'CRBP', 'EXPC', 'TLRY', 'CIDM', 'RIDE', 'SLGG', 'NTEC', 'ACB', 'IDEX', 'OCGN', 'GHSI', 'BRQS', 'CHNR', 'ELTK', 'TGC', 'BW', 'AEZS', 'NPA', 'GRNQ', 'MGNI', 'MARA', 'NMRD', 'INUV', 'GSL', 'NOVN', 'ATHE', 'BORR', 'UPWK', 'GME', 'FCEL', 'NGA', 'SYN', 'SOS', 'ADXS', 'TYME', 'LPCN', 'PHIO', 'KERN', 'NBRV', 'DMTK', 'WTER', 'CANF', 'BFT', 'SALM', 'GIK', 'AWX', 'FUV', 'DDD', 'XONE', 'SUNW', 'PECK', 'CREG', 'FTFT', 'FSR', 'CRON', 'APHA', 'EQOS', 'ACAM', 'QLGN', 'CEMI', 'HUGE', 'PIXY', 'RAVE', 'ZYNE', 'CGC', 'HEXO', 'TTNP', 'USWSW', 'KRUS', 'STAF', 'SNMP', 'ONTX', 'REED', 'PED', 'BOX', 'DPW', 'ALRN', 'SHIP', 'EKSO', 'PSAC', 'VOXX'] too legit ['NIO', 'TSLA', 'XPEL', 'ZM', 'NVDA', 'LEAP', 'LLY', 'BTWN', 'SRPT', 'FB', 'TAK', 'UVXY', 'PYPL', 'SQ', 'PENN', 'MSOS', 'EXAS', 'GRWG', 'QS', 'PLUG', 'BIDU', 'PEN', 'BA', 'SPY', 'AMD', 'INTC', 'SNAP', 'WMT', 'TWTR', 'PSTH', 'PI', 'SNX'] not available []\n"
     ]
    }
   ],
   "source": [
    "b = list(itertools.chain(*a))\n",
    "print(b)\n",
    "names = b[::2]\n",
    "mentions = b[1::2]\n",
    "print(names)\n",
    "print(mentions)\n",
    "all_names = [] \n",
    "not_listed = [] \n",
    "NA_stocks = [] ##stocks not available for trading\n",
    "too_legit = [] ##stuff that is too legit\n",
    "just_right = [] ##stuff that is just right \n",
    "just_right_mentions = [] \n",
    "for i in range(len(names)):\n",
    "    name = names[i]\n",
    "    final_name = name.strip('$')\n",
    "    all_names.append(final_name)\n",
    "    #print(final_name)\n",
    "    try:\n",
    "        b = wb.get_quote(final_name)\n",
    "        if b['status'] == 'B':\n",
    "            NA_stocks.append(final_name)\n",
    "        elif b['status'] == 'A':\n",
    "            if float(b['close'])>= 40.0 or float(b['fiftyTwoWkLow'])>= 10.0:\n",
    "                too_legit.append(final_name)\n",
    "            else:\n",
    "                just_right.append(final_name)\n",
    "                just_right_mentions.append(mentions[i])\n",
    "    except ValueError:\n",
    "        not_listed.append(final_name)\n",
    "        continue \n",
    "    except KeyError:\n",
    "        not_listed.append(final_name)\n",
    "        continue \n",
    "    #b = wb.get_bars(final_name)\n",
    "print(\"Just Right\",just_right, \"too legit\", too_legit, \"not available\", NA_stocks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USWS', 'NAKD', 'CCIV', 'TTOO', 'TMBR', 'SNDL', 'ZOM', 'CLOV', 'JUPW', 'LMFA', 'ACST', 'ISR', 'BNGO', 'AVGR', 'XSPA', 'GOVX', 'THMO', 'SIFY', 'TRXC', 'MESO', 'GEVO', 'CTRM', 'AIKI', 'DFFN', 'NSPR', 'RIOT', 'KMPH', 'NBY', 'CRBP', 'EXPC', 'TLRY', 'CIDM', 'RIDE', 'SLGG', 'NTEC', 'ACB', 'IDEX', 'OCGN', 'GHSI', 'BRQS', 'CHNR', 'ELTK', 'TGC', 'BW', 'AEZS', 'NPA', 'GRNQ', 'MGNI', 'MARA', 'NMRD', 'INUV', 'GSL', 'NOVN', 'ATHE', 'BORR', 'UPWK', 'GME', 'FCEL', 'NGA', 'SYN', 'SOS', 'ADXS', 'TYME', 'LPCN', 'PHIO', 'KERN', 'NBRV', 'DMTK', 'WTER', 'CANF', 'BFT', 'SALM', 'GIK', 'AWX', 'FUV', 'DDD', 'XONE', 'SUNW', 'PECK', 'CREG', 'FTFT', 'FSR', 'CRON', 'APHA', 'EQOS', 'ACAM', 'QLGN', 'CEMI', 'HUGE', 'PIXY', 'RAVE', 'ZYNE', 'CGC', 'HEXO', 'TTNP', 'USWSW', 'KRUS', 'STAF', 'SNMP', 'ONTX', 'REED', 'PED', 'BOX', 'DPW', 'ALRN', 'SHIP', 'EKSO', 'PSAC', 'VOXX'] [21, 21, 20, 13, 10, 9, 9, 8, 8, 7, 7, 6, 6, 6, 6, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 22, 23, 24, 25, 27, 29, 30, 31, 32, 34, 35, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 60, 65, 66, 67, 72, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 96, 97, 98, 102, 105, 108, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 151, 152, 153, 154, 155, 157, 158, 164, 165] [0.52, 0.4085, 13.2, 1.6, 0.94, 0.7087, 0.93, 14.98, 6.55, 2.45, 0.7, 1.95, 5.67, 1.45, 1.2, 5.36, 2.45, 2.85, 1.66, 11.4, 5.05, 0.258, 1.18, 1.3, 0.6655, 22.64, 6.05, 0.8076, 1.46, 12.85, 12.33, 0.779, 27.11, 3.24, 4.4, 10.44, 2.88, 2.49, 0.72, 1.34, 1.61, 5.24, 1.52, 3.93, 0.6548, 12.52, 2.58, 25.56, 23.36, 4.06, 0.657, 14.9, 1.01, 1.53, 0.9312, 38.0, 19.94, 15.75, 24.06, 0.6543, 2.29, 0.6616, 1.72, 1.67, 3.1, 6.12, 2.88, 33.58, 1.1, 1.91, 15.55, 1.42, 14.42, 3.2, 17.15, 27.59, 13.69, 7.95, 14.82, 6.64, 5.47, 14.91, 9.8, 9.05, 17.34, 11.53, 3.58, 5.32, 2.14, 2.97, 0.9045, 3.45, 31.47, 5.06, 3.48, 0.055, 18.75, 0.8454, 0.763, 0.69, 0.8061, 1.41, 17.79, 4.1, 1.92, 0.82, 9.2, 12.0, 14.76] [1658464.0, 129099614.0, 2853291.0, 3870225.0, 2207692.0, 277049501.0, 98826863.0, 6844715.0, 450700.0, 8056484.0, 30106015.0, 7178604.0, 52233018.0, 15365484.0, 5631236.0, 2265814.0, 659964.0, 924349.0, 9510144.0, 766336.0, 24536741.0, 34451615.0, 7055355.0, 1453394.0, 4163505.0, 23049641.0, 135902.0, 1389201.0, 5253731.0, 802734.0, 21624058.0, 10931075.0, 7918901.0, 692539.0, 872575.0, 40513785.0, 38078834.0, 44046181.0, 18040831.0, 3144438.0, 252721.0, 44851.0, 1505478.0, 260989.0, 3204464.0, 772207.0, 3025144.0, 5126799.0, 31132843.0, 35840.0, 3568967.0, 86315.0, 15230076.0, 2301284.0, 4742469.0, 2409666.0, 10224384.0, 59673286.0, 2134414.0, 8090735.0, 3276611.0, 7658266.0, 601314.0, 2116462.0, 604540.0, 2342825.0, 530534.0, 495001.0, 1774666.0, 463615.0, 7977587.0, 589936.0, 1758039.0, 270819.0, 2874679.0, 9871951.0, 485285.0, 6542372.0, 1039487.0, 600877.0, 8290682.0, 12510269.0, 5857617.0, 11980434.0, 1065742.0, 1212785.0, 1121701.0, 1546466.0, 940242.0, 876375.0, 1212818.0, 864169.0, 7117255.0, 5621869.0, 1630304.0, 188148.0, 58154.0, 4619428.0, 662614.0, 22680586.0, 900405.0, 1192641.0, 1894221.0, 10822964.0, 2452046.0, 7507621.0, 125765.0, 217613.0, 333099.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "twitter_rank = [] \n",
    "friday_close = [] \n",
    "friday_volume = [] \n",
    "three_month_volume = [] \n",
    "fifty_two_week_low = [] \n",
    "fifty_two_week_high = [] \n",
    "\n",
    "for i in range(len(just_right)):\n",
    "    for x in range(len(all_names)):\n",
    "        if just_right[i] == all_names[x]:\n",
    "            x = x+1\n",
    "            twitter_rank.append(x)\n",
    "    quote = wb.get_quote(just_right[i])\n",
    "    friday_close.append(float(quote['close']))\n",
    "    friday_volume.append(float(quote['volume']))\n",
    "    three_month_volume.append(float(quote['avgVol3M']))\n",
    "    fifty_two_week_low.append(float(quote['fiftyTwoWkLow']))\n",
    "    fifty_two_week_high.append(float(quote['fiftyTwoWkHigh']))\n",
    "print(just_right,just_right_mentions, twitter_rank,friday_close,three_month_volume)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################set export file path#######\n",
    "workbook  = xlsxwriter.Workbook('/Users/jpjansen/Documents/1_TESTTEST_WeBullStats.xlsx')\n",
    "############################################################################\n",
    "worksheet = workbook.add_worksheet()\n",
    "worksheet.write(0,0,'Stock Symbol')\n",
    "worksheet.write(0,1,'Twitter Rank')\n",
    "worksheet.write(0,2,'Mentions')\n",
    "worksheet.write(0,3,'Friday Close')\n",
    "worksheet.write(0,4, 'Friday Vol')\n",
    "worksheet.write(0,5, '3 Month Vol')\n",
    "worksheet.write(0,6, '52 Week Min')\n",
    "worksheet.write(0,7, '52 Week Max')\n",
    "for i in range(len(just_right)):\n",
    "    worksheet.write(i+1, 0,just_right[i])\n",
    "    worksheet.write(i+1, 1,twitter_rank[i])\n",
    "    worksheet.write(i+1, 2,just_right_mentions[i])\n",
    "    worksheet.write(i+1, 3,friday_close[i])\n",
    "    worksheet.write(i+1, 4,friday_volume[i])\n",
    "    worksheet.write(i+1, 5,three_month_volume[i])\n",
    "    worksheet.write(i+1, 6,fifty_two_week_low[i])\n",
    "    worksheet.write(i+1, 7,fifty_two_week_high[i])\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
